---
layout: blog
program: true
background-image: http://spring.io/img/homepage/icon-spring-framework.svg
title:  "从泰勒级数的角度理解机器学习中无约束迭代优化问题"
date:   2020-04-03
background-image: http://spring.io/img/homepage/icon-spring-framework.svg
category: 机器学习
tags:
- machine learning
- 优化分析
---

>众多机器学习算法推导的背后最终大都可以归纳为一个优化问题，即寻找满足使预测值与实际值之间损失最小函数模型的参数问题。在理论分析中有些优化问题的求解可以一步得到结果，但是实际工程中由于数据量和维数往往比较大，计算机需要通过不断迭代的形式才能最快速的得到模型的最优解。本文从泰勒级数的角度分析无约束迭代优化问题中常见的梯度下降法和拟牛顿法。

## 要分析的问题
- [x] 为什么一阶导数为0的点是可能局部极值点的解空间
- [x] 负梯度的方向为什么是下降速度最快的方向
- [x] 无约束迭代优化问题模型归纳

## 标量空间和向量空间中泰勒级数展开
高等数学里学过对标量空间的一点$$x_0+\alpha$$泰勒级数展开式可以写为（这里只保留到二阶导数项）：
<div>$$f(x_0+\alpha)\approx f(x_0)+f^\prime(x_0)\alpha+\frac 12f^{\prime\prime}(x_0)\alpha^2$$</div>
我们在分析展开式的一阶导数项时将二阶导项数先忽略不计，假设$$f(x_0)$$是函数的局部极小值点，那么就是说存在任意的$$\alpha$$（可正可负）使得：
<div>$$f(x_0+\alpha)\ge f(x_0)$$ 即 $$f(x_0)+f^\prime(x_0)\alpha \ge f(x_0)$$</div>
所以由夹逼法可以得出$$f^\prime(x_0)=0$$，同理如果假设$$f(x_0)$$是函数的局部极大值点也可以得到上述结论。到这里我们就解决了要分析的第一个问题，为什么一阶导数为0的点是可能局部极值点的解空间。换句话说，一节导数为0的点是函数取极值点的必要不充分条件。我们称一阶导数为0的点为平稳点，它可能是局部极大值点、局部极小值点还可能是鞍点。这时再将展开式的二阶项考虑进去可以看出当一阶导数为0，二阶导数$$f^{\prime\prime}(x_0) > 0$$时有$$f(x_0+\alpha)\ge f(x_0)$$，所以$$f(x_0)$$取极小值。同理可得到当$$f^{\prime\prime}(x_0) < 0$$时，$$f(x_0)$$取极大值。

参照标量空间的泰勒级数展开，可以写出向量空间某一维度的泰勒级数展开式。在写其展开式之前先引出梯度和Hessian矩阵。对应于标量空间的一阶导数，梯度就是向量空间在各维度方向的一阶偏导，Hessian矩阵就是向量空间中在各个维度方向上的二阶偏导其可以对应于标量空间中的二阶导数。所以在向量$$\vec x_k+\vec \alpha_k$$处泰勒级数展开式可以写为:
<div>$$f(\vec x_k+\vec \alpha_k) \approx f(\vec \alpha_k )+g^\intercal (\alpha_k)\alpha_k+\frac 12 \alpha_k^\intercal H(\alpha_k)\alpha_k$$</div>
<div>其中：</div>
<div>$$g(\alpha_k)=
\begin{pmatrix}
\frac{\partial f}{\partial x_1}\\
  \frac{\partial f}{\partial x_2}\\
  \frac{\partial f}{\partial x_3}\\
  \vdots\\
  \frac{\partial f}{\partial x_n}\\
\end{pmatrix}
$$
$$
  H(\alpha_k)=
  \begin{pmatrix}
  f_{x_1x_1}&f_{x_1x_2}&\cdots&f_{x_1x_n}\\
  f_{x_2x_1}&f_{x_2x_2}&\cdots&f_{x_2x_n}\\
  \vdots&\vdots&\vdots&\vdots\\
  f_{x_nx_1}&f_{x_nx_2}&\cdots&f_{x_nx_n}\\
\end{pmatrix}
$$
</div>
<div>上述通过泰勒级数推理了在标量空间中函数取极值点的必要条件和充分条件。这里对于向量空间为了好理解可以简单的类比得出向量空间取局部极值点的条件，这里直接给出，不做具体理论推导。由上式可知Hessian矩阵为对称阵，当其为正定矩阵时，函数在向量空间存在极小值点。
</div>
##迭代法分析无约束优化模型
无约束问题可以理解为在没有解空间外部限制条件的前提下求解函数模型的极值问题，在机器学习的很多算法中一般都会转化为求解函数模型的极小值问题。对于无约束模型的解空间优化分析，计算机一般采用擅长的迭代法在有限的迭代次数内找到最优解。迭代过程中向量空间变化方向的可以表示为$$x_{k+1} = x_k+\lambda d_k$$，所以$$d_k$$就是向量空间的搜索方向，不同的无约束优化分析方法之间的区别就在于向量空间的搜索方向不同。所以该类优化问题求解可以简单的归结为如下几步：

-初始化：在向量空间中随机初始化起始点$$\x_{0}$$，迭代步长$$\lambda$$，
停止迭代阈值$$\alpha$$

-确定搜索方向：计算迭代搜索方向$$d_{x}$$，更新迭代方向$$x_{k+1} = x_k+\lambda d_k$$

-迭代终止条件：当搜索方向向量$$||d_{k}|| \lt \alpha$$时表明迭代步骤已经走不动了此时迭代截止，得到此时函数的局部极值。

上述介绍了不同优化迭代算法之间的主要区别在于搜索向量$$d_{k}$$的定义不同，下面将分别介绍常用的梯度下降法、牛顿法、拟牛顿法中的搜索向量
###梯度下降法
###牛顿法
###拟牛顿法