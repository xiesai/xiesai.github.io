---
layout: blog
program: true
background-image: http://spring.io/img/homepage/icon-spring-framework.svg
title:  "从泰勒级数的角度理解机器学习中无约束迭代优化问题"
date:   2020-04-03
background-image: http://spring.io/img/homepage/icon-spring-framework.svg
category: 机器学习
tags:
- machine learning
- 优化分析
---

>众多机器学习算法推导的背后最终大都可以归纳为一个优化问题，即寻找满足使预测值与实际值之间损失最小函数模型的参数问题。在理论分析中有些优化问题的求解可以一步得到结果，但是实际工程中由于数据量和维数往往比较大，计算机需要通过不断迭代的形式才能最快速的得到模型的最优解。本文从泰勒级数的角度分析无约束迭代优化问题中常见的梯度下降法和拟牛顿法。

## 要分析的问题
- [x] 为什么一阶导数为0的点是可能局部极值点的解空间
- [x] 负梯度的方向为什么是下降速度最快的方向
- [x] 无约束迭代优化问题模型归纳

## 标量空间和向量空间中泰勒级数展开
高等数学里学过对标量空间的一点\\$\\$(x_0+\alpha)\\$\\$泰勒级数展开式可以写为（这里只保留到二阶导数项）：
<div style="display:hidden;">$$f(x_0+\alpha)\approx f(x_0)+f^\prime(x_0)\alpha+\frac 12f^{\prime\prime}(x_0)\alpha^2$$</div>
我们写分析展开式的一阶导数项将二阶导项数先忽略不计，我们假设如果$$f(x_0)$$是函数的局部极小值点，那么就是存在任意的$$\alpha$$（可正可负）使得：
$$f(x_0+\alpha)\ge f(x_0)$$ 即 $$f(x_0)+f^\prime(x_0)\alpha \ge f(x_0)$$
所有由夹逼法可以得出$$f^\prime(x_0)=0$$，同理如果假设$$f(x_0)$$是函数的局部极大值点也可以得到上述结论。到这里我们就解决了要分析的第一个问题，为什么一阶导数为0的点事可能局部极值点的解空间。换句话说，一节导数为0的点是函数取极值点的必要不充分条件。我们称一节导数为0的点为平稳点，它可能是局部极大值点、局部极小值点还可能是鞍点。

参照标量空间的泰勒级数展开，可以写出向量空间某一维度的泰勒级数展开式。在写其展开式之前先引出梯度和Hessian矩阵。对应于标量空间的一阶导数，梯度就是向量空间在各维度方向的一阶偏导，Hessian矩阵就是向量空间中在各个维度方向上的二阶偏导其可以对应于标量空间中的二阶导数。所以在向量$$\vec x_k+\vec \alpha_k$$处泰勒级数展开式可以写为：
$$f(\vec x_k+\vec \alpha_k) \approx f(\vec \alpha_k )+g^\intercal (\alpha_k)\alpha_k+\frac 12 \alpha_k^\intercal H(\alpha_k)\alpha_k$$
其中：